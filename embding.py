# ==============================================================================
# B√ÄI T·∫¨P L·ªöN FACE ID - SCRIPT CH·∫†Y SONG SONG TR√äN NHI·ªÄU M√ÅY (SHARDING)
# ==============================================================================

# --- 1. C·∫§U H√åNH M√ÅY (B·∫†N CH·ªà C·∫¶N S·ª¨A D√íNG N√ÄY) ---
TOTAL_MACHINES = 5      # T·ªïng s·ªë m√°y b·∫°n ƒë·ªãnh d√πng
CURRENT_PART = 0        # <--- S·ª¨A S·ªê N√ÄY: M√°y 1 ƒëi·ªÅn 0, M√°y 2 ƒëi·ªÅn 1, ..., M√°y 5 ƒëi·ªÅn 4

# --- 2. C√ÄI ƒê·∫∂T M√îI TR∆Ø·ªúNG ---
import os
import time
import math
import glob
import random
import numpy as np
from tqdm import tqdm

# K·∫øt n·ªëi Google Drive ƒë·ªÉ l∆∞u k·∫øt qu·∫£
from google.colab import drive
drive.mount('/content/drive')

# C√†i th∆∞ vi·ªán
print("‚è≥ ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán...")
!pip install -q deepface kagglehub

from deepface import DeepFace
import kagglehub

# T·∫°o th∆∞ m·ª•c l∆∞u tr·ªØ tr√™n Drive
SAVE_DIR = '/content/drive/MyDrive/Colab Notebooks/Hoc May CH37/BTL'
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)
    print(f"‚úÖ ƒê√£ t·∫°o/t√¨m th·∫•y th∆∞ m·ª•c l∆∞u tr·ªØ: {SAVE_DIR}")

# --- 3. T·∫¢I DATASET T·ª∞ ƒê·ªòNG ---
print("\n‚è≥ ƒêang t·∫£i Dataset t·ª´ Kaggle (c√≥ th·ªÉ m·∫•t 1-2 ph√∫t)...")
try:
    path = kagglehub.dataset_download("yakhyokhuja/ms1m-arcface-dataset")
    print("‚úÖ Dataset ƒë√£ t·∫£i t·∫°i:", path)

    # X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n (Fix l·ªói c·∫•u tr√∫c th∆∞ m·ª•c)
    possible_path = os.path.join(path, "ms1m-arcface")
    if os.path.exists(possible_path):
        DATASET_ROOT = possible_path
    else:
        DATASET_ROOT = path # Fallback
    print(f"üìÇ Th∆∞ m·ª•c g·ªëc ch·ª©a ·∫£nh l√†: {DATASET_ROOT}")

except Exception as e:
    print(f"‚ùå L·ªói t·∫£i Dataset: {e}")
    # D·ª´ng ch∆∞∆°ng tr√¨nh n·∫øu kh√¥ng c√≥ data
    raise e

# --- 4. CHIA D·ªÆ LI·ªÜU (SHARDING LOGIC) ---
print(f"\nü§ñ ƒêANG KH·ªûI T·∫†O M√ÅY S·ªê {CURRENT_PART + 1} / {TOTAL_MACHINES}...")

# L·∫•y t·∫•t c·∫£ th∆∞ m·ª•c ID v√† S·∫ÆP X·∫æP (B·∫Øt bu·ªôc sort ƒë·ªÉ ƒë·ªìng b·ªô gi·ªØa c√°c m√°y)
all_folders = sorted(glob.glob(os.path.join(DATASET_ROOT, "*")))
total_ids = len(all_folders)

if total_ids == 0:
    raise ValueError("‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c ·∫£nh n√†o! Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n.")

# T√≠nh to√°n ph·∫ßn vi·ªác c·ªßa m√°y n√†y
chunk_size = math.ceil(total_ids / TOTAL_MACHINES)
start_index = CURRENT_PART * chunk_size
end_index = min((CURRENT_PART + 1) * chunk_size, total_ids)

my_folders = all_folders[start_index : end_index]

print(f"üìå NHI·ªÜM V·ª§: X·ª≠ l√Ω t·ª´ ID th·ª© {start_index} ƒë·∫øn {end_index}")
print(f"üìä T·ªïng s·ªë ID m√°y n√†y c·∫ßn l√†m: {len(my_folders)}")

# --- 5. L·ªåC ·∫¢NH V√Ä CHU·∫®N B·ªä LIST ---
MIN_IMGS = 6
GALLERY_SIZE = 5

gallery_paths, gallery_labels = [], []
probe_paths, probe_labels = [], []

print("‚è≥ ƒêang qu√©t v√† l·ªçc ·∫£nh...")
for folder_path in my_folders:
    img_files = glob.glob(os.path.join(folder_path, "*.jpg"))

    if len(img_files) >= MIN_IMGS:
        id_name = os.path.basename(folder_path)
        # Shuffle ƒë·ªÉ l·∫•y ng·∫´u nhi√™n
        random.shuffle(img_files)

        # Gallery: L·∫•y ƒë√∫ng 5 ·∫£nh
        g_imgs = img_files[:GALLERY_SIZE]
        gallery_paths.extend(g_imgs)
        gallery_labels.extend([id_name] * len(g_imgs))

        # Probe: L·∫•y t·ªëi ƒëa 2 ·∫£nh c√≤n l·∫°i ƒë·ªÉ test (Tr√°nh l·∫•y qu√° nhi·ªÅu g√¢y ch·∫≠m)
        p_imgs = img_files[GALLERY_SIZE : GALLERY_SIZE + 2]
        probe_paths.extend(p_imgs)
        probe_labels.extend([id_name] * len(p_imgs))

print(f"‚úÖ ƒê√£ chu·∫©n b·ªã xong list ·∫£nh:")
print(f"   - Gallery: {len(gallery_paths)} ·∫£nh")
print(f"   - Probe:   {len(probe_paths)} ·∫£nh")
print(f"   - T·ªïng c·ªông: {len(gallery_paths) + len(probe_paths)} ·∫£nh")
print(f"‚è±Ô∏è ∆Ø·ªõc t√≠nh th·ªùi gian ch·∫°y: ~{(len(gallery_paths) + len(probe_paths))/36000:.1f} gi·ªù (v·ªõi t·ªëc ƒë·ªô 10it/s)")

# --- 6. H√ÄM T·∫†O VECTOR V√Ä L∆ØU ---
def l2_normalize(x):
    norm = np.linalg.norm(x)
    return x / norm if norm != 0 else x

def process_and_save(img_paths, labels, name_prefix):
    if len(img_paths) == 0: return

    print(f"\nüöÄ ƒêang x·ª≠ l√Ω t·∫≠p {name_prefix} (Part {CURRENT_PART})...")
    vectors = []
    valid_labels = []

    # Batch processing loop
    for path, label in tqdm(zip(img_paths, labels), total=len(img_paths)):
        try:
            # --- CORE LOGIC ---
            # detector_backend='skip' : TƒÉng t·ªëc t·ªëi ƒëa
            obj = DeepFace.represent(
                img_path=path,
                model_name="ArcFace",
                enforce_detection=False,
                detector_backend="skip"
            )
            vec = obj[0]["embedding"]
            vectors.append(l2_normalize(np.array(vec)))
            valid_labels.append(label)
        except:
            continue

    # L∆∞u file
    save_name_vec = f'{name_prefix}_vectors_part_{CURRENT_PART}.npy'
    save_name_lbl = f'{name_prefix}_labels_part_{CURRENT_PART}.npy'

    np.save(os.path.join(SAVE_DIR, save_name_vec), np.array(vectors))
    np.save(os.path.join(SAVE_DIR, save_name_lbl), np.array(valid_labels))

    print(f"üéâ ƒê√É L∆ØU TH√ÄNH C√îNG: {save_name_vec}")

# --- 7. CH·∫†Y TH·ª∞C T·∫æ ---
# Ch·∫°y Probe tr∆∞·ªõc (Nhanh - ƒë·ªÉ test)
process_and_save(probe_paths, probe_labels, "probe")

# Ch·∫°y Gallery sau (L√¢u)
print("\nüí§ Ngh·ªâ 5 gi√¢y tr∆∞·ªõc khi ch·∫°y Gallery...")
time.sleep(5)
process_and_save(gallery_paths, gallery_labels, "gallery")

print("\nüéØ M√ÅY N√ÄY ƒê√É HO√ÄN T·∫§T! B·∫†N C√ì TH·ªÇ ƒê√ìNG TAB.")